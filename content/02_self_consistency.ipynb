{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Self-Consistency\n",
    "\n",
    "This notebook demonstrates self-consistency prompting: running the same LLM call multiple times and aggregating results to get more reliable sentiment analysis.\n",
    "\n",
    "**Prerequisites:** Set `INSERT_API_KEY_HERE` before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI, OpenAI\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "client = OpenAI(api_key=\"INSERT_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Structured Output Models\n",
    "\n",
    "Pydantic models for bank sentiment analysis with constrained sentiment values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(str, Enum):\n",
    "    positive = \"positive\"\n",
    "    neutral = \"neutral\"\n",
    "    negative = \"negative\"\n",
    "\n",
    "\n",
    "class BankSentiment(BaseModel):\n",
    "    bank_name: str = Field(description=\"Name of the bank.\")\n",
    "    sentiment: Sentiment = Field(description=\"Sentiment towards the bank.\")\n",
    "    sentiment_explanation: str = Field(description=\"Justification of the chosen sentiment.\")\n",
    "\n",
    "\n",
    "class NewsAnalysis(BaseModel):\n",
    "    bank_sentiments: List[BankSentiment] = Field(description=\"Sentiment analysis for each mentioned bank.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article & Prompts\n",
    "\n",
    "A synthetic news article with deliberately ambiguous sentiment signals for three fictional banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_article = \"\"\"\n",
    "In a week of turbulent trading, three mid-sized lenders \\u2014 Aurora Capital Bank, GreenStone Community Bank, and Horizon Meridian Bank \\u2014 found themselves at the center of conflicting analyst commentary.\n",
    "\n",
    "Aurora Capital Bank surprised markets with stronger-than-expected quarterly earnings, helped by robust fee income and tight cost control. Several analysts praised its \\u201cdisciplined risk management\\u201d and \\u201cimpressive digital onboarding experience,\\u201d noting that customer satisfaction scores hit record highs. However, regulators have quietly raised concerns about Aurora\\u2019s growing exposure to speculative commercial real estate, warning that \\u201ca downturn could quickly erode its currently healthy capital buffers.\\u201d Some investors remain wary, describing the bank\\u2019s growth strategy as \\u201caggressive and potentially reckless\\u201d despite the upbeat headline numbers.\n",
    "\n",
    "GreenStone Community Bank, which focuses on sustainable lending and local businesses, announced a modest profit decline. Management framed the results as a \\u201cnecessary short-term sacrifice\\u201d to expand green lending programs and offer temporary fee waivers to struggling small firms. Environmental groups applauded GreenStone\\u2019s \\u201cgenuine commitment to social impact,\\u201d and local entrepreneurs credit the bank with \\u201ckeeping their doors open.\\u201d Yet the stock fell after an influential brokerage downgraded GreenStone to \\u201cunderperform,\\u201d citing \\u201cweak profitability, slow digital transformation, and a lack of operational discipline.\\u201d While long-term customers remain loyal, some shareholders question whether the bank is \\u201ctoo idealistic to compete effectively.\\u201d\n",
    "\n",
    "Horizon Meridian Bank delivered mixed results, beating expectations on revenue but missing consensus on net income due to one-off restructuring charges. Executives called the quarter a \\u201cturning point,\\u201d emphasizing early success in reducing non-performing loans and modernizing back-office systems. A major rating agency reaffirmed Horizon\\u2019s investment-grade status and highlighted its \\u201cimproving risk profile.\\u201d At the same time, employee unions criticized recent branch closures as \\u201cshort-sighted cost cutting,\\u201d and social media sentiment turned sharply negative after reports of extended call-center wait times. While a few market commentators described Horizon as \\u201cquietly fixing its problems,\\u201d others warned that \\u201cmanagement credibility is still fragile and customer trust far from restored.\\u201d\n",
    "\n",
    "Overall, investors and observers are sharply divided on all three institutions: each bank can point to clear signs of progress and stability, yet each is also dogged by lingering doubts, conflicting signals, and narratives that can be read as either cautiously optimistic or quietly alarming.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a senior financial text analysis engine.\n",
    "\n",
    "Constraints:\n",
    "- Focus on banks only, ignore other companies.\n",
    "- Be conservative with positive sentiment; require clearly favorable language.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Objectives:\n",
    "- Extract all banks mentioned in a news article.\n",
    "- Determine sentiment (positive / neutral / negative) towards each bank.\n",
    "\n",
    "Article:\n",
    "{doc_article}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run\n",
    "\n",
    "A single LLM call to see the baseline result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    response_format=NewsAnalysis,\n",
    ")\n",
    "response = completion.choices[0].message.parsed\n",
    "\n",
    "for bs in response.bank_sentiments:\n",
    "        print(f\"{bs.bank_name}: {str(bs.sentiment)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Consistency: Multiple Runs\n",
    "\n",
    "Run the same prompt N times and aggregate the sentiment counts per bank to see how consistent the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "sentiment_counts = defaultdict(Counter)\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_format=NewsAnalysis,\n",
    "    )\n",
    "\n",
    "    analysis: NewsAnalysis = completion.choices[0].message.parsed\n",
    "\n",
    "    for bs in analysis.bank_sentiments:\n",
    "        bank = bs.bank_name.strip()\n",
    "        sentiment_counts[bank][bs.sentiment.value] += 1\n",
    "\n",
    "for bank, counts in sentiment_counts.items():\n",
    "    print(f\"Bank: {bank}\")\n",
    "    for sentiment, count in counts.items():\n",
    "        print(f\"  {sentiment}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
